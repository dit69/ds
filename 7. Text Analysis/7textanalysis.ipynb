{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNHfFtDNa9AtIQobxCRns75"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"BLWLQr3_2T2w","executionInfo":{"status":"ok","timestamp":1745918494428,"user_tz":-330,"elapsed":4297,"user":{"displayName":"Aman prakash Singh","userId":"10795255997562671376"}}},"outputs":[],"source":["import nltk\n","import re\n","from nltk.tokenize import sent_tokenize\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer"]},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt_tab')\n","nltk.download('averaged_perceptron_tagger_eng')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B7NViWTF2cx6","executionInfo":{"status":"ok","timestamp":1745918530308,"user_tz":-330,"elapsed":1569,"user":{"displayName":"Aman prakash Singh","userId":"10795255997562671376"}},"outputId":"1bee2268-0c46-42a4-897c-b82a85217bbd"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["text= '''Tokenization is the first step in text analytics. The\n","process of breaking down a text paragraph into smaller chunks\n","such as words or sentences is called Tokenization.'''"],"metadata":{"id":"PEnI3XKB2otB","executionInfo":{"status":"ok","timestamp":1745918578199,"user_tz":-330,"elapsed":5,"user":{"displayName":"Aman prakash Singh","userId":"10795255997562671376"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Tokenization"],"metadata":{"id":"uqs427wt27k_"}},{"cell_type":"code","source":["#Sentence Tokenization\n","tokenized_text= sent_tokenize(text)\n","print(tokenized_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gfYLtDDH20w7","executionInfo":{"status":"ok","timestamp":1745918653471,"user_tz":-330,"elapsed":9,"user":{"displayName":"Aman prakash Singh","userId":"10795255997562671376"}},"outputId":"37063fe9-9ff2-49fa-d215-7e3c4163701d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['Tokenization is the first step in text analytics.', 'The\\nprocess of breaking down a text paragraph into smaller chunks\\nsuch as words or sentences is called Tokenization.']\n"]}]},{"cell_type":"code","source":["#Word Tokenization\n","tokenized_word=word_tokenize(text)\n","print(tokenized_word)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OOuazkkZ3FAa","executionInfo":{"status":"ok","timestamp":1745918668180,"user_tz":-330,"elapsed":74,"user":{"displayName":"Aman prakash Singh","userId":"10795255997562671376"}},"outputId":"4f7825e4-5bf7-44b1-b6ab-6b001f74eef1"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics', '.', 'The', 'process', 'of', 'breaking', 'down', 'a', 'text', 'paragraph', 'into', 'smaller', 'chunks', 'such', 'as', 'words', 'or', 'sentences', 'is', 'called', 'Tokenization', '.']\n"]}]},{"cell_type":"markdown","source":["Removing Punctuations and Stop Word"],"metadata":{"id":"pEOIWrk33eLD"}},{"cell_type":"code","source":["# print stop words of English\n","stop_words=set(stopwords.words(\"english\"))\n","print(stop_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v6WkGc_i3Kuj","executionInfo":{"status":"ok","timestamp":1745918680920,"user_tz":-330,"elapsed":13,"user":{"displayName":"Aman prakash Singh","userId":"10795255997562671376"}},"outputId":"a61b4689-05e6-40fe-8168-f2fdbcba6642"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["{'didn', 'do', 'who', \"shan't\", 'while', 'both', 'then', 'why', 't', 'yours', 'where', \"it's\", 'off', 'their', 'o', 'other', \"we'll\", \"you're\", 'having', \"she'd\", \"they'd\", \"should've\", 'you', 'no', 'i', 'her', 'such', 'between', \"couldn't\", 'each', 'these', 'are', 'same', \"aren't\", 'does', 'or', \"mightn't\", 'before', 'most', 'once', 'under', 'but', 'any', 'so', 'what', 'some', 'below', 'was', 'his', 'll', 'won', 'over', 'wouldn', 'here', 'out', 'yourselves', 'aren', \"i'd\", \"she's\", \"weren't\", 'if', \"didn't\", \"that'll\", \"he's\", 'until', \"hasn't\", 'mightn', 'up', 'itself', \"she'll\", \"you'd\", 'because', 'again', \"we'd\", \"he'd\", 'have', 'they', 'am', 'theirs', 'how', 'than', 've', 'your', 'it', 'own', \"it'd\", 'which', 'ours', 'against', 'down', 'only', 'ourselves', \"we've\", 'ma', 'she', \"you've\", \"they've\", 'our', \"they're\", 'more', 'a', 'when', 'with', 'as', 'wasn', 'shan', 'will', 'during', \"it'll\", 'should', 'shouldn', 'this', 'be', 'being', 'ain', 'doesn', 'few', 'further', \"we're\", \"doesn't\", 'the', 'too', 'm', \"wouldn't\", \"you'll\", 'weren', 'been', 'nor', 'is', 'for', 'through', 'to', 'yourself', 'from', \"don't\", 'just', 'themselves', \"needn't\", 'at', 'there', \"they'll\", 're', \"wasn't\", 'him', 'hadn', 'of', 'mustn', 'an', 'me', 'all', 'that', 'don', 'had', 'doing', \"i'll\", 's', 'those', 'himself', 'can', 'in', 'on', 'herself', 'its', \"hadn't\", 'after', 'did', 'were', 'very', 'y', 'isn', \"i've\", 'now', \"i'm\", \"isn't\", 'my', 'above', \"he'll\", 'hers', 'has', \"mustn't\", 'couldn', 'needn', 'myself', 'hasn', 'd', 'into', \"won't\", 'and', \"shouldn't\", 'we', 'he', \"haven't\", 'haven', 'them', 'not', 'by', 'about', 'whom'}\n"]}]},{"cell_type":"code","source":["text= \"How to remove stop words with NLTK library in Python?\"\n","text= re.sub('[^a-zA-Z]', ' ',text)\n","tokens = word_tokenize(text.lower())\n","filtered_text=[]\n","for w in tokens:\n","  if w not in stop_words:\n","    filtered_text.append(w)\n","print(\"Tokenized Sentence:\",tokens)\n","print(\"Filterd Sentence:\",filtered_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MxIq1HuG3N2O","executionInfo":{"status":"ok","timestamp":1745918715809,"user_tz":-330,"elapsed":13,"user":{"displayName":"Aman prakash Singh","userId":"10795255997562671376"}},"outputId":"cbe37574-6e80-4fdb-8132-4ee160154191"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized Sentence: ['how', 'to', 'remove', 'stop', 'words', 'with', 'nltk', 'library', 'in', 'python']\n","Filterd Sentence: ['remove', 'stop', 'words', 'nltk', 'library', 'python']\n"]}]},{"cell_type":"markdown","source":["Perform Stemming"],"metadata":{"id":"I0zUHnrl3ita"}},{"cell_type":"code","source":["e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n","ps =PorterStemmer()\n","for w in e_words:\n","  rootWord=ps.stem(w)\n","  print(rootWord)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"np5Ov0Pd3UGo","executionInfo":{"status":"ok","timestamp":1745918811378,"user_tz":-330,"elapsed":9,"user":{"displayName":"Aman prakash Singh","userId":"10795255997562671376"}},"outputId":"039ff6f1-599b-431e-b18b-57605f439ed6"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["wait\n","wait\n","wait\n","wait\n"]}]},{"cell_type":"markdown","source":["Perform Lemmatization"],"metadata":{"id":"Lqwkwj5R303L"}},{"cell_type":"code","source":["wordnet_lemmatizer = WordNetLemmatizer()\n","text = \"studies studying cries cry\"\n","tokenization = nltk.word_tokenize(text)\n","for w in tokenization:\n","  print(\"Lemma for {} is {}\".format(w,wordnet_lemmatizer.lemmatize(w)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lILWHxHO3ptk","executionInfo":{"status":"ok","timestamp":1745918874592,"user_tz":-330,"elapsed":4236,"user":{"displayName":"Aman prakash Singh","userId":"10795255997562671376"}},"outputId":"03074973-c23e-4ce0-e16f-b2d017d44d90"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Lemma for studies is study\n","Lemma for studying is studying\n","Lemma for cries is cry\n","Lemma for cry is cry\n"]}]},{"cell_type":"markdown","source":["Apply POS Tagging to text"],"metadata":{"id":"rlRmeaCY4Jop"}},{"cell_type":"code","source":["data=\"The pink sweater fit her perfectly\"\n","words=word_tokenize(data)\n","for word in words:\n","  print(nltk.pos_tag([word]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6JPi5pVQ38Fn","executionInfo":{"status":"ok","timestamp":1745918923318,"user_tz":-330,"elapsed":329,"user":{"displayName":"Aman prakash Singh","userId":"10795255997562671376"}},"outputId":"578e82db-e19b-49c9-b87c-879608584a3a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[('The', 'DT')]\n","[('pink', 'NN')]\n","[('sweater', 'NN')]\n","[('fit', 'NN')]\n","[('her', 'PRP$')]\n","[('perfectly', 'RB')]\n"]}]},{"cell_type":"markdown","source":["TF-IDF Representation of Documents"],"metadata":{"id":"fQtSEdR65JP6"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer"],"metadata":{"id":"L1PgqqwY4Tq_","executionInfo":{"status":"ok","timestamp":1745919204602,"user_tz":-330,"elapsed":4,"user":{"displayName":"Aman prakash Singh","userId":"10795255997562671376"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Multiple sample documents (for IDF to make sense)\n","documents = [\n","    \"Natural Language Processing is an interesting field.\",\n","    \"Machine learning and AI are transforming the world.\",\n","    \"Python is widely used for data science and AI.\",\n","    \"Natural Language Processing and machine learning are core parts of AI.\"\n","]"],"metadata":{"id":"i1gNnvxZ5NpK","executionInfo":{"status":"ok","timestamp":1745919228273,"user_tz":-330,"elapsed":417,"user":{"displayName":"Aman prakash Singh","userId":"10795255997562671376"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Create TF-IDF vectorizer\n","vectorizer = TfidfVectorizer(stop_words='english')\n","\n","# Transform documents\n","tfidf_matrix = vectorizer.fit_transform(documents)"],"metadata":{"id":"S9zxWymQ5TV8","executionInfo":{"status":"ok","timestamp":1745919256422,"user_tz":-330,"elapsed":4,"user":{"displayName":"Aman prakash Singh","userId":"10795255997562671376"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Show feature names and TF-IDF matrix\n","print(\"\\nFeature Names:\", vectorizer.get_feature_names_out())\n","print(\"\\nTF-IDF Matrix:\\n\", tfidf_matrix.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mv9NVRNm5aUE","executionInfo":{"status":"ok","timestamp":1745919282435,"user_tz":-330,"elapsed":210,"user":{"displayName":"Aman prakash Singh","userId":"10795255997562671376"}},"outputId":"51d17b29-3623-4793-c3de-ee8d1b53274e"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Feature Names: ['ai' 'core' 'data' 'field' 'interesting' 'language' 'learning' 'machine'\n"," 'natural' 'parts' 'processing' 'python' 'science' 'transforming' 'used'\n"," 'widely' 'world']\n","\n","TF-IDF Matrix:\n"," [[0.         0.         0.         0.50867187 0.50867187 0.40104275\n","  0.         0.         0.40104275 0.         0.40104275 0.\n","  0.         0.         0.         0.         0.        ]\n"," [0.33406745 0.         0.         0.         0.         0.\n","  0.41263976 0.41263976 0.         0.         0.         0.\n","  0.         0.52338122 0.         0.         0.52338122]\n"," [0.27448674 0.         0.43003652 0.         0.         0.\n","  0.         0.         0.         0.         0.         0.43003652\n","  0.43003652 0.         0.43003652 0.43003652 0.        ]\n"," [0.27178692 0.42580674 0.         0.         0.         0.33571092\n","  0.33571092 0.33571092 0.33571092 0.42580674 0.33571092 0.\n","  0.         0.         0.         0.         0.        ]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"JnQ2b-wo5gp5"},"execution_count":null,"outputs":[]}]}